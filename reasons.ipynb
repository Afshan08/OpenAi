{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88ad7cf3",
   "metadata": {},
   "source": [
    "## **First Question Answer Reason**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd5dbe",
   "metadata": {},
   "source": [
    "- **A.** `\"Your friend's next class is computer science.\"`  \n",
    "  *Because the context clearly provides the answer, and the agent is instructed to use it.*\n",
    "\n",
    "- **B.** `\"I'm sorry, I do not have access to your friend's schedule.\"`  \n",
    "  *Because plain string context is not automatically parsed or structured in the Agent SDK.*\n",
    "\n",
    "- **C.** *Runtime error due to invalid context type.*  \n",
    "  *Because the SDK only accepts dataclass-based context objects.*\n",
    "\n",
    "- **D.** `\"The agent answeringagent is not valid because it was not defined with a context model.\"`  \n",
    "  *Because you didn't specify `Agent[str]` or `Agent[UserContext]`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698496bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef9e0d51",
   "metadata": {},
   "source": [
    "# **Correct Answer: B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff811d",
   "metadata": {},
   "source": [
    "## **Corrected Version of the code of question 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc343a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from agents.run import RunConfig\n",
    "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, RunContextWrapper, function_tool\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_api_key = \"your_gemini_api_key_here\"  # Replace with your actual Gemini API key\n",
    "client = AsyncOpenAI(\n",
    "    api_key=gemini_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "model = OpenAIChatCompletionsModel(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    openai_client=client\n",
    ")\n",
    "\n",
    "config = RunConfig(\n",
    "    model=model,\n",
    "    model_provider=client,\n",
    "    tracing_disabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14523792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umme-habiba is your friend and she has PIAIC quarter two class.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FriendClass():\n",
    "    friend_name:str\n",
    "    class_name:str\n",
    "\n",
    "\n",
    "\n",
    "## code:\n",
    "async def main():\n",
    "    ctx = FriendClass(friend_name=\"Umme-habiba\", class_name=\"PIAIC quarter two class\")\n",
    "    \n",
    "    @function_tool\n",
    "    async def fetch_friends_data(wrapper: RunContextWrapper[FriendClass]) -> str:\n",
    "        \"\"\"\n",
    "            Returns the friends name and class by accessing the context object.\n",
    "            No arguments are required.\n",
    "        \"\"\"\n",
    "        return f\"{wrapper.context.friend_name} is my friend having {wrapper.context.class_name} class.\"\n",
    "\n",
    "        \n",
    "    answeringagent = Agent[FriendClass](\n",
    "        name=\"answering_agent\",\n",
    "        handoff_description=\"Specialist agent for answering questions about friends' classes.\",\n",
    "        instructions=(\n",
    "            \"You are given a context object with fields 'friend_name' and 'class_name'. \"\n",
    "            \"Use the tool 'fetch_friends_data' to answer questions about this context.\"\n",
    "            \"If asked about the friends class, use the available tool to fetch it. do not ask it just fetch the data The tool works using context only and does not take user input\"\n",
    "        ),\n",
    "        model=model,\n",
    "        tools=[fetch_friends_data],\n",
    "    )\n",
    "    Triage_agent = Agent[FriendClass](\n",
    "        name=\"orchestrator_agent\",\n",
    "        instructions=\"You are going to handoff the task to the appropriate agent.\",\n",
    "        handoffs=[answeringagent],\n",
    "        model=model,\n",
    "    )\n",
    "    \n",
    "        \n",
    "    answer = await Runner.run(Triage_agent, \"Tell me what is the class my friend Umme habiba will attend today? \", context=ctx )\n",
    "    print(answer.final_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23e3295a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user is 47 years old.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from agents import Agent, RunContextWrapper, Runner, function_tool\n",
    "\n",
    "@dataclass\n",
    "class UserInfo:  \n",
    "    name: str\n",
    "    uid: int\n",
    "\n",
    "@function_tool\n",
    "async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str: \n",
    "    \"\"\"\n",
    "        Returns the user's age by accessing the context object.\n",
    "        No arguments are required.\n",
    "    \"\"\" \n",
    "    return f\"User {wrapper.context.name} is 47 years old\"\n",
    "\n",
    "async def main():\n",
    "    user_info = UserInfo(name=\"John\", uid=123)\n",
    "\n",
    "    agent = Agent[UserInfo](  \n",
    "        name=\"Assistant\",\n",
    "        instructions=\"If asked about the user's age, use the available tool to fetch it. The tool works using context only and does not take user input\",\n",
    "        tools=[fetch_user_age],\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    result = await Runner.run(  \n",
    "        starting_agent=agent,\n",
    "        input=\"What is the age of the user?\",\n",
    "        context=user_info,\n",
    "    )\n",
    "\n",
    "    print(result.final_output)  \n",
    "    # The user John is 47 years old.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c389e8b",
   "metadata": {},
   "source": [
    "### **Important things that I learned about context management in between agents**\n",
    "\n",
    "- 1 thing is that agent does not knows how to transfer string as context to the agnets therefore we should make dataclass\n",
    "- data classes are made to let the agents know that the context you are going to recive would be of this type\n",
    "- also the type of context is not enough for the agent to understand how to use the agent we will have to use a function that has the RunContextWrapper class as a type of the input and gives str output so that the agent can use it and answer the question\n",
    "- Also the agent should know that they will not be needing to input the arguments because that is directly given by the class called RunContextWrapper to the function and the agent can directly call it\n",
    "- Runner.run or any function when will have the context it should be the type given to the agent with agent_name[T] of any type made by dataclass and make sure the information are filled in the required variables of the custom dataclass you made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6b2e2",
   "metadata": {},
   "source": [
    "### Question 2 **Code does not works!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57dff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \n",
    "    # there is no function_tool decorator here, so the tool will not be used. and the agent\n",
    "    # does not knows how to call the tool.\n",
    "    async def answer_question() -> str:\n",
    "        \"\"\"If the agent can not answer the question, it will use this tool to answer it.\"\"\"\n",
    "        print(\"Answering question using the tool...\")\n",
    "\n",
    "    agent = Agent(\n",
    "        model=model,\n",
    "        name=\"example_agent\",\n",
    "        instructions=\"You are an agent that can answer questions about a specific topic.\",\n",
    "        tools=[answer_question]\n",
    "    )\n",
    "    \n",
    "    result = await Runner.run(agent, \"What is my father's name?\")\n",
    "    \n",
    "    print(result.final_output)\n",
    "    # The tool will be used to answer the question.\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10617ea5",
   "metadata": {},
   "source": [
    "### *Two Better Ways*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b237306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\"\"\"\n",
    "    The first is obvious just use the function decorator tool\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "    The second way is to import the FunctionTool class and use it directly. because\n",
    "    the function_tool decorator also analyzes the function signatures it into the FunctionTool class\n",
    "    to make it useable.\n",
    "\"\"\"\n",
    "\n",
    "## code:\n",
    "from agents import FunctionTool\n",
    "\n",
    "async def main():\n",
    "    class AnswerQuestion(BaseModel):\n",
    "        question: str = Field(..., description=\"The question to answer.\")\n",
    "    \n",
    "    async def answer_question(context :RunContextWrapper[Any], *args) -> str:\n",
    "        \"\"\"A tool to answer questions. If the agent can not answer the question, it will use this tool to answer it.\"\"\"\n",
    "        parsed = AnswerQuestion.model_validate_json(args)\n",
    "        return f\"You asked: {parsed.question}\"\n",
    "\n",
    "\n",
    "    answering_tool = FunctionTool(\n",
    "        name=\"answer_question\",\n",
    "        description=\"A tool to answer questions. call it whenever the agent does not know tha answer. It also does not needs any arguments.S\",\n",
    "        params_json_schema=AnswerQuestion.model_json_schema(),\n",
    "        on_invoke_tool=answer_question,\n",
    "    )\n",
    "    \n",
    "    agent = Agent(\n",
    "        model=model,\n",
    "        name=\"example_agent\",\n",
    "        instructions=\"You are an agent that will only call the tool and answer the users question.\",\n",
    "        tools=[answering_tool]\n",
    "    )\n",
    "    \n",
    "    result = await Runner.run(agent, \"What is my father's name?\")\n",
    "    \n",
    "    print(result.final_output)\n",
    "    # The tool will be used to answer the question.\n",
    "if __name__ == \"__main__\":\n",
    "    await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e569249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from agents import RunContextWrapper, FunctionTool\n",
    "\n",
    "\n",
    "\n",
    "def do_some_work(data: str) -> str:\n",
    "    return \"done\"\n",
    "\n",
    "\n",
    "class FunctionArgs(BaseModel):\n",
    "    username: str\n",
    "    age: int\n",
    "\n",
    "async def main():\n",
    "    async def run_function(ctx: RunContextWrapper[Any], args: str) -> str:\n",
    "        parsed = FunctionArgs.model_validate_json(args)\n",
    "        return do_some_work(data=f\"{parsed.username} is {parsed.age} years old\")\n",
    "\n",
    "\n",
    "    tool = FunctionTool(\n",
    "        name=\"process_user\",\n",
    "        description=\"Processes extracted user data\",\n",
    "        params_json_schema=FunctionArgs.model_json_schema(),\n",
    "        on_invoke_tool=run_function,\n",
    "    )\n",
    "    \n",
    "    agent_tools = Agent[FunctionArgs](\n",
    "        model=model,\n",
    "        name=\"example_agent\",\n",
    "        instructions=\"You are an agent that can process user data. and answer questions about it. You can use the tool 'process_user' to process the user data.\",\n",
    "        tools=[tool]\n",
    "    )\n",
    "    \n",
    "    result = await Runner.run(\n",
    "        agent_tools,\n",
    "        \"Process the user data for John and tell me his age.\",\n",
    "        context=RunContextWrapper(context=FunctionArgs(username=\"John\", age=30))\n",
    "    )\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed7f74ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalServerError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m tool = FunctionTool(\n\u001b[32m     24\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mprocess_user\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m     description=\u001b[33m\"\u001b[39m\u001b[33mProcesses extracted user data\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     params_json_schema=FunctionArgs.model_json_schema(),\n\u001b[32m     27\u001b[39m     on_invoke_tool=run_function,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m agent_tools = Agent[FunctionArgs](\n\u001b[32m     31\u001b[39m     model=model,\n\u001b[32m     32\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mexample_agent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     33\u001b[39m     instructions=\u001b[33m\"\u001b[39m\u001b[33mYou are an agent that can process user data. and answer questions about it. You can use the tool \u001b[39m\u001b[33m'\u001b[39m\u001b[33mprocess_user\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to process the user data.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m     tools=[tool]\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(\n\u001b[32m     38\u001b[39m     agent_tools,\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mProcess the user data for John and tell me his age.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     40\u001b[39m     context=RunContextWrapper(context=FunctionArgs(username=\u001b[33m\"\u001b[39m\u001b[33mJohn\u001b[39m\u001b[33m\"\u001b[39m, age=\u001b[32m30\u001b[39m))\n\u001b[32m     41\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.final_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ehsan Afridi\\.conda\\envs\\EmailAgent\\Lib\\site-packages\\agents\\run.py:219\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[39m\n\u001b[32m    214\u001b[39m logger.debug(\n\u001b[32m    215\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    216\u001b[39m )\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    220\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    221\u001b[39m             starting_agent,\n\u001b[32m    222\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    223\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    224\u001b[39m             copy.deepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[32m    225\u001b[39m             context_wrapper,\n\u001b[32m    226\u001b[39m         ),\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    228\u001b[39m             agent=current_agent,\n\u001b[32m    229\u001b[39m             all_tools=all_tools,\n\u001b[32m    230\u001b[39m             original_input=original_input,\n\u001b[32m    231\u001b[39m             generated_items=generated_items,\n\u001b[32m    232\u001b[39m             hooks=hooks,\n\u001b[32m    233\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    234\u001b[39m             run_config=run_config,\n\u001b[32m    235\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    236\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    237\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    238\u001b[39m         ),\n\u001b[32m    239\u001b[39m     )\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    241\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    242\u001b[39m         agent=current_agent,\n\u001b[32m    243\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    251\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    252\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ehsan Afridi\\.conda\\envs\\EmailAgent\\Lib\\site-packages\\agents\\run.py:787\u001b[39m, in \u001b[36mRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    785\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    788\u001b[39m     agent,\n\u001b[32m    789\u001b[39m     system_prompt,\n\u001b[32m    790\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    791\u001b[39m     output_schema,\n\u001b[32m    792\u001b[39m     all_tools,\n\u001b[32m    793\u001b[39m     handoffs,\n\u001b[32m    794\u001b[39m     context_wrapper,\n\u001b[32m    795\u001b[39m     run_config,\n\u001b[32m    796\u001b[39m     tool_use_tracker,\n\u001b[32m    797\u001b[39m     previous_response_id,\n\u001b[32m    798\u001b[39m )\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    801\u001b[39m     agent=agent,\n\u001b[32m    802\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    811\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    812\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ehsan Afridi\\.conda\\envs\\EmailAgent\\Lib\\site-packages\\agents\\run.py:946\u001b[39m, in \u001b[36mRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    943\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m    944\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m    947\u001b[39m     system_instructions=system_prompt,\n\u001b[32m    948\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    949\u001b[39m     model_settings=model_settings,\n\u001b[32m    950\u001b[39m     tools=all_tools,\n\u001b[32m    951\u001b[39m     output_schema=output_schema,\n\u001b[32m    952\u001b[39m     handoffs=handoffs,\n\u001b[32m    953\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m    954\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    955\u001b[39m     ),\n\u001b[32m    956\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    957\u001b[39m )\n\u001b[32m    959\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ehsan Afridi\\.conda\\envs\\EmailAgent\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:62\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     48\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m     previous_response_id: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     56\u001b[39m ) -> ModelResponse:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     58\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     59\u001b[39m         model_config=model_settings.to_json_dict() | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._client.base_url)},\n\u001b[32m     60\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     61\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     63\u001b[39m             system_instructions,\n\u001b[32m     64\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     65\u001b[39m             model_settings,\n\u001b[32m     66\u001b[39m             tools,\n\u001b[32m     67\u001b[39m             output_schema,\n\u001b[32m     68\u001b[39m             handoffs,\n\u001b[32m     69\u001b[39m             span_generation,\n\u001b[32m     70\u001b[39m             tracing,\n\u001b[32m     71\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     72\u001b[39m         )\n\u001b[32m     74\u001b[39m         first_choice = response.choices[\u001b[32m0\u001b[39m]\n\u001b[32m     75\u001b[39m         message = first_choice.message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ehsan Afridi\\.conda\\envs\\EmailAgent\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:264\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream)\u001b[39m\n\u001b[32m    258\u001b[39m store = ChatCmplHelpers.get_store_param(\u001b[38;5;28mself\u001b[39m._get_client(), model_settings)\n\u001b[32m    260\u001b[39m stream_options = ChatCmplHelpers.get_stream_options_param(\n\u001b[32m    261\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_client(), model_settings, stream=stream\n\u001b[32m    262\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_client().chat.completions.create(\n\u001b[32m    265\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    266\u001b[39m     messages=converted_messages,\n\u001b[32m    267\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    268\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    269\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    270\u001b[39m     frequency_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.frequency_penalty),\n\u001b[32m    271\u001b[39m     presence_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.presence_penalty),\n\u001b[32m    272\u001b[39m     max_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    273\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    274\u001b[39m     response_format=response_format,\n\u001b[32m    275\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    276\u001b[39m     stream=stream,\n\u001b[32m    277\u001b[39m     stream_options=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(stream_options),\n\u001b[32m    278\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(store),\n\u001b[32m    279\u001b[39m     reasoning_effort=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(reasoning_effort),\n\u001b[32m    280\u001b[39m     extra_headers={**HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    281\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    282\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    283\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    284\u001b[39m )\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, ChatCompletion):\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ehsan Afridi\\.conda\\envs\\EmailAgent\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2028\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1985\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1986\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1987\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2025\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2026\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2027\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2029\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2030\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2031\u001b[39m             {\n\u001b[32m   2032\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2033\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2034\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2035\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2036\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2037\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2038\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2039\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2040\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2041\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2042\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2043\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2044\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2045\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2046\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2047\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2048\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2049\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2050\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2051\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2052\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2053\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2054\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2055\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2056\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2057\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2058\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2059\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2060\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2061\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2062\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2063\u001b[39m             },\n\u001b[32m   2064\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2065\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2066\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2067\u001b[39m         ),\n\u001b[32m   2068\u001b[39m         options=make_request_options(\n\u001b[32m   2069\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2070\u001b[39m         ),\n\u001b[32m   2071\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2072\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2073\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2074\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ehsan Afridi\\.conda\\envs\\EmailAgent\\Lib\\site-packages\\openai\\_base_client.py:1748\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1734\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1735\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1736\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1743\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1744\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1745\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1746\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1747\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1748\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ehsan Afridi\\.conda\\envs\\EmailAgent\\Lib\\site-packages\\openai\\_base_client.py:1555\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1552\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1554\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1555\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1557\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mInternalServerError\u001b[39m: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a0eee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m.final_output)\n",
      "\u001b[31mNameError\u001b[39m: name 'result' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71b7d5b6",
   "metadata": {},
   "source": [
    "print(result.final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EmailAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
